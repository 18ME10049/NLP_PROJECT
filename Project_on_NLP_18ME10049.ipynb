{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project_on_NLP_18ME10049.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/18ME10049/NLP_PROJECT/blob/main/Project_on_NLP_18ME10049.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyJ25uz0kSaw"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# Project  on Natural Language Processing\n",
        "\n",
        "## Duration : 15th Nov - 10 Dec, 2020\n",
        "\n",
        "### Instructor : Prof. Sudeshna Sarkar\n",
        "\n",
        "### Teaching Assistants : Alapan Kuila, Aniruddha Roy, Anusha Potnuru, Uppada Vishnu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ao1nhg9RknmF"
      },
      "source": [
        "The central idea of this Project is to explore various language models specifically LSTM based and transformer. We will explore how the size of the model effects the sequence generated. We will see both character based and word based models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXdkhxZAXnTW"
      },
      "source": [
        "# Word Based LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbU5DRolXseI"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Bidirectional\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "import numpy\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import keras"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2NR3RFFYOT8"
      },
      "source": [
        "Do basic pre processing which includes lowering etc\n",
        "Check the dataset and apply suitable preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQvfF2NjXxGj"
      },
      "source": [
        "# Load the data and preprocess data and store corpus in raw_text\n",
        "# Load the data and preprocess data and store corpus in raw_text\n",
        "with open('corpus (1).txt','r',encoding=\"utf8\") as file:\n",
        "    data = file.read()\n",
        "file.close()\n",
        "\n",
        "raw_text = data.lower()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPHuRkEPZH_W"
      },
      "source": [
        "import string\n",
        "def  clean_my_corpos(doc):\n",
        "\n",
        "    doc = doc.replace('”', '\"')\n",
        "    doc = doc.replace('“', '\"')\n",
        "    doc = doc.replace('_',\"\")\n",
        "    doc = doc.replace(';','.')\n",
        "    doc = doc.replace('—','')\n",
        "    return doc"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdXcWFxu6nEc"
      },
      "source": [
        "my_text = clean_my_corpos(raw_text)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eug68GOecM8Z"
      },
      "source": [
        "# Hyperparameters of the model\n",
        "vocab_size = 3000 # choose based on statistics\n",
        "oov_tok = '<OOV>'\n",
        "embedding_dim = 100\n",
        "padding_type='post'\n",
        "trunc_type='post'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWNBOlJ5cQym"
      },
      "source": [
        "# tokenize sentences\n",
        "tokenizer = Tokenizer(oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts([my_text])\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "tokens = tokenizer.texts_to_sequences([my_text])[0]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOx877nxf8Kp",
        "outputId": "3b65c1de-e13c-4d12-c49d-da4bda4e5227"
      },
      "source": [
        "print(\"Size of Word_index is : \", 1+len(word_index))\n",
        "print(\"Total number of words :\",len(tokens))\n",
        "print(\"Total Unique words : \",len(set(tokens)))\n",
        "vocab_size =  1+len(word_index)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of Word_index is :  2764\n",
            "Total number of words : 26570\n",
            "Total Unique words :  2762\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykfI4FrwdyJe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "792b1910-41dc-4751-901d-cdecec4412b9"
      },
      "source": [
        "dataX = []\n",
        "dataY = []\n",
        "seq_length = 50\n",
        "for i in range(0, len(tokens)-seq_length-1 , 1):\n",
        "  seq_in = tokens[i:i + seq_length]\n",
        "  seq_out = tokens[i + seq_length]\n",
        "\n",
        "  if seq_out==1: #Skip samples where target word is <OOV>\n",
        "    continue\n",
        "    \n",
        "  dataX.append(seq_in)\n",
        "  dataY.append(seq_out)\n",
        "\n",
        "N = len(dataX)\n",
        "print (\"Total training data size: \", N)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total training data size:  26519\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbyVDpXMimY4",
        "outputId": "047b91e3-0706-49cc-9e78-f6e447e62319"
      },
      "source": [
        "print(dataX[0],dataY[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[305, 10, 36, 2, 101, 695, 11, 14, 279, 4, 104, 27, 475, 8, 349, 75, 17, 423, 19, 2, 1065, 3, 8, 350, 131, 4, 52, 137, 56, 601, 6, 22, 834, 63, 2, 326, 17, 423, 14, 835, 23, 7, 22, 43, 696, 56, 1066, 12, 7, 3] 29\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJmGr1xId8cO"
      },
      "source": [
        "X = numpy.array(dataX)\n",
        "\n",
        "# one hot encode the output variable\n",
        "y = numpy.array(dataY)\n",
        "y = np_utils.to_categorical(dataY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QPApRA-d9JV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebee96ad-576b-4d2c-aeaf-18a5ed8bbddb"
      },
      "source": [
        "#with embedding\n",
        "embedding_dim = 50\n",
        "#seq_length = 50\n",
        "# Deep Neural network with one embedding layer , one Bidirection LSTM , one Dense layer with softmax activation \n",
        "\n",
        "# model = keras.Sequential([\n",
        "#     keras.layers.Embedding(vocab_size, embedding_dim, input_length=seq_length),\n",
        "#     keras.layers.Bidirectional(keras.layers.LSTM(64 , activation=tahh , use_bias=True , bias_initializer='zeros' , )),\n",
        "#     keras.layers.Dense(vocab_size, activation='softmax')\n",
        "# ])\n",
        "\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "print(model.summary())\n",
        "\n",
        "# model compilation with crossentropy loss and adam optimization using metrics accuracy.\n",
        "# compile model\n",
        "# model.compile(loss='categorical_crossentropy',\n",
        "#               optimizer='adam',\n",
        "#               metrics=['accuracy'])\n",
        "\n",
        "# model summary\n",
        "# model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 50, 50)            138200    \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 50, 100)           60400     \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 2764)              279164    \n",
            "=================================================================\n",
            "Total params: 568,264\n",
            "Trainable params: 568,264\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oqkh8uFsZiJN",
        "outputId": "ac28f12c-9b82-4573-a611-a70384b66998"
      },
      "source": [
        "#Model 2\n",
        "\n",
        "model2 = Sequential()\n",
        "model2.add(Embedding(vocab_size, 100, input_length=seq_length))\n",
        "model2.add(LSTM(100, return_sequences=True))\n",
        "model2.add(LSTM(100))\n",
        "model2.add(Dense(100, activation='relu'))\n",
        "model2.add(Dense(vocab_size, activation='softmax'))\n",
        "print(model2.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, 50, 100)           276400    \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, 50, 100)           80400     \n",
            "_________________________________________________________________\n",
            "lstm_6 (LSTM)                (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 2764)              279164    \n",
            "=================================================================\n",
            "Total params: 726,464\n",
            "Trainable params: 726,464\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PSpI1s5NPBU"
      },
      "source": [
        "# compile model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftMFaTg_Zp_G"
      },
      "source": [
        "# compile model2\n",
        "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0Si7TUUNW41"
      },
      "source": [
        "model.fit(X, y, batch_size=128, epochs=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxYLols_ZucM"
      },
      "source": [
        "model2.fit(X, y, batch_size=128, epochs=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7-nbsUyqOuO"
      },
      "source": [
        "from keras import models \n",
        "model1_with_less_epoch = models.load_model('my_model_for_text_gen.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puvVzYhxW1w0"
      },
      "source": [
        "#model.save('model.h5')\n",
        "model2.save('my_model_for_text_gen_with_60_accuracy.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwVn0-OfXVGI"
      },
      "source": [
        "from keras import models \n",
        "model1_with_epoch = models.load_model('my_model_for_text_gen.h5')\n",
        "History_model2_with_100_epoch = models.load_model('my_model_for_text_gen_with_60_accuracy.h5') # Accuracy >= 50%"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mg9WSEwYeMAH"
      },
      "source": [
        "reverse_word_map = tokenizer.index_word ## Create word to idx map using tokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_mhL0J0eQku"
      },
      "source": [
        "def generate_seq(model, seq_length, seed_text, n_words):\n",
        "\tresult = list()\n",
        "\tin_text = seed_text\n",
        "\t# generate a fixed number of words\n",
        "\tfor _ in range(n_words):\n",
        "\t\t# encode the text as integer\n",
        "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "\t\t# truncate sequences to a fixed length\n",
        "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "\t\t# predict probabilities for each word\n",
        "\t\tyhat = np.argmax(model.predict(encoded),axis=-1)\n",
        "        #yhat = np.argmax(model.predict(encoded),axis=-1)\n",
        "\t\t# map predicted word index to word\n",
        "\t\tout_word = ''\n",
        "\t\tfor word, index in tokenizer.word_index.items():\n",
        "\t\t\tif index == yhat:\n",
        "\t\t\t\tout_word = word\n",
        "\t\t\t\tbreak\n",
        "\t\t# append to input\n",
        "\t\tin_text += ' ' + out_word\n",
        "\t\tresult.append(out_word)\n",
        "\treturn ' '.join(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZ_NTQezeWYO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2866d4e-c497-4a2b-b9a6-6e837cf02cec"
      },
      "source": [
        "# pick a random seed\n",
        "\n",
        "start_index = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start_index]  #Random_sentence\n",
        "input_str = ' '.join([reverse_word_map[value] for value in pattern])\n",
        "real_answer = []\n",
        "for i in range(start_index+50 , start_index + 50 + 50):\n",
        "    real_answer.append(tokens[i])\n",
        "\n",
        "real_sequance = ' '.join([reverse_word_map[value] for value in real_answer])\n",
        "\n",
        "print(\"\\nGiven Seed or Input: \\n\", input_str)\n",
        "print(\"\\nReal Answer according to corpos will be: \\n\", real_sequance)\n",
        "\n",
        "answer1 = generate_seq(model1_with_epoch, seq_length, input_str, 100)\n",
        "answer2 = generate_seq(History_model2_with_100_epoch, seq_length, input_str, 100)\n",
        "\n",
        "print(\"\\nPredict Sequence using model1: \\n\",answer1)\n",
        "print(\"\\nPredict Sequence using model2: \\n\",answer2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Given Seed or Input: \n",
            " to notice this last remark of course they were said the dormouse well in this answer so confused poor alice that she let the dormouse go on for some time without interrupting it they were learning to draw the dormouse went on yawning and rubbing its eyes for it was\n",
            "\n",
            "Real Answer according to corpos will be: \n",
            " getting very sleepy and they drew all manner of thingseverything that begins with an m why with an m said alice why not said the march hare alice was silent the dormouse had closed its eyes by this time and was going off into a doze but on being pinched\n",
            "\n",
            "Predict Sequence using model1: \n",
            " a little timidly said alice alice was a little house and began to cry and she went on and she had been to the seaside all the queen and she had been a little house on it and the mock turtle went on and began whistling so she had been to the seaside all the queen and the queen was very likely said the king and the moral of the panther received pigs continued the king and the queen was very likely said alice and the mock turtle went on and the moral of that she went on and she\n",
            "\n",
            "Predict Sequence using model2: \n",
            " a knot said alice rather timidly to the part she went up and began bowing and the flurry of the lizard’s boyand the fact i keep her chin about a foot welleh stupid but alice had a right nervous certainly happened and what she got to alice and sometimes out of swimming off with the top of the sky twinkle always thenalways to look across that dormouse treading on the fire who alice was the white rabbit trotting slowly and cheerfully she comes to work dripping a footman call it as she had grown up it said the hatter i\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-x8W-OSVgNi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d19507af-fc1f-4847-e7d8-961c4e1fa8ea"
      },
      "source": [
        "input_str = \"The boy laughed at the fright he had caused. This time, the villagers left angrily. The third day, as the boy went up\\\n",
        " the small hill, he suddenly saw a wolf attacking his sheep. He cried as hard as he could, “Wolf! Wolf! Wolf!”, but not \\\n",
        " a single villager came to help him. The villagers thought that he was trying to fool them again and did not come to rescue \\\n",
        " him or his sheep.\"\n",
        "\n",
        "# Use first 50 tokens from given input_str as input.(Use tokenizer to split to take first 50)\n",
        "print(generate_seq(History_model2_with_100_epoch,seq_length, input_str , 10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "weeks i only twice round her spectacles and walked at\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5_R5Tngo3_D"
      },
      "source": [
        "# Character based LSTM Model 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zZaHsejo57p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02730cac-448f-4892-f56d-1ff516e8fd6a"
      },
      "source": [
        "# User the preprocess data and create raw_text\n",
        "import keras\n",
        "import keras.utils\n",
        "from keras import utils as np_utils\n",
        "# create mapping of unique characters to integers\n",
        "chars = sorted(list(set(my_text)))\n",
        "char_tokenizer2 = Tokenizer(num_words=None, char_level=True, oov_token='UNK')\n",
        "char_tokenizer2.fit_on_texts(my_text)\n",
        "\n",
        "char_to_int =  char_tokenizer2.word_index\n",
        "mapping = dict((c, i) for i, c in enumerate(chars))\n",
        "print(\"Number of char in vocabulary :\" ,len(char_to_int))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of char in vocabulary : 43\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkVVDbump0Wg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba1ee63b-c741-40c3-811c-700994511460"
      },
      "source": [
        "# Print the total characters and character vacob size\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(char_to_int)\n",
        "print(\"n_chars : \",n_chars)\n",
        "print(\"n_vocab : \",n_vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n_chars :  142477\n",
            "n_vocab :  43\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aVserymqE1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7862d2b-df49-4a66-e030-87e03bfb2799"
      },
      "source": [
        "'''\n",
        "Prepare dataset where the input is sequence of 100 characters and target is next character.\n",
        "'''\n",
        "tokens2 = char_tokenizer2.texts_to_sequences([raw_text])[0]\n",
        "seq_length = 50\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, 50000 - seq_length, 1):\n",
        "    seq_in = tokens2[i:i+seq_length]\n",
        "    seq_out = tokens2[i + seq_length]\n",
        "    \n",
        "    if seq_out == 1:  # skip the oov_token.\n",
        "        continue\n",
        "    \n",
        "    dataX.append(seq_in)\n",
        "    dataY.append(seq_out)\n",
        "\n",
        "\n",
        "n_patterns = len(dataX)\n",
        "print (\"Total Patterns: \", n_patterns)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Patterns:  49063\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ic4yf4hNqc7T"
      },
      "source": [
        "# reshape X to be [samples, time steps, features]\n",
        "X = numpy.array(dataX)\n",
        "# one hot encode the output variable\n",
        "dataY = numpy.array(dataY)\n",
        "y = np_utils.to_categorical(dataY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvawnjFVqhMi"
      },
      "source": [
        "embedding_dim =100\n",
        "max_length =50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ek5DqNeTqkAZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c8feccd-4ed5-41f1-a477-935c697cba22"
      },
      "source": [
        "n_vocab = n_vocab + 1\n",
        "char_model1 = Sequential()\n",
        "char_model1.add(Embedding(n_vocab, embedding_dim, input_length=max_length))\n",
        "char_model1.add(LSTM(128))\n",
        "char_model1.add(Dropout(0.2))\n",
        "char_model1.add(Dense(y.shape[1], activation='softmax'))\n",
        "char_model1.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "char_model1.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_6 (Embedding)      (None, 50, 100)           4400      \n",
            "_________________________________________________________________\n",
            "lstm_7 (LSTM)                (None, 128)               117248    \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 44)                5676      \n",
            "=================================================================\n",
            "Total params: 127,324\n",
            "Trainable params: 127,324\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5WDzdruS7qf",
        "outputId": "f5c703d3-71b0-4b3f-ac50-b528381c7e80"
      },
      "source": [
        "char_model1.fit(X, y, epochs=50, batch_size=128)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "384/384 [==============================] - 71s 184ms/step - loss: 2.2330\n",
            "Epoch 2/50\n",
            "384/384 [==============================] - 70s 182ms/step - loss: 2.0660\n",
            "Epoch 3/50\n",
            "384/384 [==============================] - 69s 180ms/step - loss: 1.9384\n",
            "Epoch 4/50\n",
            "384/384 [==============================] - 70s 181ms/step - loss: 1.8392\n",
            "Epoch 5/50\n",
            "384/384 [==============================] - 70s 183ms/step - loss: 1.7609\n",
            "Epoch 6/50\n",
            "384/384 [==============================] - 70s 182ms/step - loss: 1.7019\n",
            "Epoch 7/50\n",
            "384/384 [==============================] - 70s 182ms/step - loss: 1.6465\n",
            "Epoch 8/50\n",
            "384/384 [==============================] - 70s 181ms/step - loss: 1.6022\n",
            "Epoch 9/50\n",
            "384/384 [==============================] - 70s 182ms/step - loss: 1.5616\n",
            "Epoch 10/50\n",
            "384/384 [==============================] - 71s 185ms/step - loss: 1.5236\n",
            "Epoch 11/50\n",
            "384/384 [==============================] - 70s 182ms/step - loss: 1.4922\n",
            "Epoch 12/50\n",
            "384/384 [==============================] - 69s 181ms/step - loss: 1.4627\n",
            "Epoch 13/50\n",
            "384/384 [==============================] - 70s 182ms/step - loss: 1.4332\n",
            "Epoch 14/50\n",
            "384/384 [==============================] - 69s 180ms/step - loss: 1.4122\n",
            "Epoch 15/50\n",
            "384/384 [==============================] - 70s 183ms/step - loss: 1.3852\n",
            "Epoch 16/50\n",
            "384/384 [==============================] - 69s 181ms/step - loss: 1.3668\n",
            "Epoch 17/50\n",
            "384/384 [==============================] - 69s 181ms/step - loss: 1.3406\n",
            "Epoch 18/50\n",
            "384/384 [==============================] - 70s 181ms/step - loss: 1.3202\n",
            "Epoch 19/50\n",
            "384/384 [==============================] - 70s 182ms/step - loss: 1.3024\n",
            "Epoch 20/50\n",
            "384/384 [==============================] - 70s 182ms/step - loss: 1.2826\n",
            "Epoch 21/50\n",
            "384/384 [==============================] - 70s 182ms/step - loss: 1.2628\n",
            "Epoch 22/50\n",
            "384/384 [==============================] - 70s 181ms/step - loss: 1.2441\n",
            "Epoch 23/50\n",
            "384/384 [==============================] - 69s 180ms/step - loss: 1.2321\n",
            "Epoch 24/50\n",
            "384/384 [==============================] - 71s 185ms/step - loss: 1.2120\n",
            "Epoch 25/50\n",
            "384/384 [==============================] - 70s 181ms/step - loss: 1.2011\n",
            "Epoch 26/50\n",
            "384/384 [==============================] - 70s 181ms/step - loss: 1.1820\n",
            "Epoch 27/50\n",
            "384/384 [==============================] - 70s 182ms/step - loss: 1.1695\n",
            "Epoch 28/50\n",
            "384/384 [==============================] - 70s 183ms/step - loss: 1.1548\n",
            "Epoch 29/50\n",
            "384/384 [==============================] - 69s 180ms/step - loss: 1.1373\n",
            "Epoch 30/50\n",
            "384/384 [==============================] - 69s 180ms/step - loss: 1.1245\n",
            "Epoch 31/50\n",
            "384/384 [==============================] - 70s 182ms/step - loss: 1.1137\n",
            "Epoch 32/50\n",
            "384/384 [==============================] - 70s 183ms/step - loss: 1.0962\n",
            "Epoch 33/50\n",
            "384/384 [==============================] - 71s 185ms/step - loss: 1.0905\n",
            "Epoch 34/50\n",
            "384/384 [==============================] - 70s 183ms/step - loss: 1.0770\n",
            "Epoch 35/50\n",
            "384/384 [==============================] - 71s 184ms/step - loss: 1.0609\n",
            "Epoch 36/50\n",
            "384/384 [==============================] - 70s 182ms/step - loss: 1.0527\n",
            "Epoch 37/50\n",
            "384/384 [==============================] - 70s 183ms/step - loss: 1.0443\n",
            "Epoch 38/50\n",
            "384/384 [==============================] - 71s 184ms/step - loss: 1.0321\n",
            "Epoch 39/50\n",
            "384/384 [==============================] - 70s 182ms/step - loss: 1.0207\n",
            "Epoch 40/50\n",
            "384/384 [==============================] - 71s 184ms/step - loss: 1.0122\n",
            "Epoch 41/50\n",
            "384/384 [==============================] - 70s 182ms/step - loss: 1.0028\n",
            "Epoch 42/50\n",
            "384/384 [==============================] - 70s 182ms/step - loss: 0.9902\n",
            "Epoch 43/50\n",
            "384/384 [==============================] - 70s 183ms/step - loss: 0.9822\n",
            "Epoch 44/50\n",
            "384/384 [==============================] - 70s 183ms/step - loss: 0.9762\n",
            "Epoch 45/50\n",
            "384/384 [==============================] - 69s 181ms/step - loss: 0.9661\n",
            "Epoch 46/50\n",
            "384/384 [==============================] - 70s 182ms/step - loss: 0.9595\n",
            "Epoch 47/50\n",
            "384/384 [==============================] - 71s 185ms/step - loss: 0.9538\n",
            "Epoch 48/50\n",
            "384/384 [==============================] - 69s 181ms/step - loss: 0.9462\n",
            "Epoch 49/50\n",
            "384/384 [==============================] - 70s 181ms/step - loss: 0.9394\n",
            "Epoch 50/50\n",
            "384/384 [==============================] - 69s 180ms/step - loss: 0.9292\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7ee5836908>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8S8_tk7oJew"
      },
      "source": [
        "char_model1.save('char_based_model1.h5')\n",
        "#model.save('my_model_for_text_gen.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekXYhzeUrGaq"
      },
      "source": [
        "#implement mapping of integer to character\n",
        "int_to_char = char_tokenizer2.index_word\n",
        "# int_to_char"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OixPrw6vq15j"
      },
      "source": [
        "'''\n",
        "Complete code below to get the generated string using the model.\n",
        "'''\n",
        "def generate_seq_char_based(model, mapping, seq_length, seed_text, n_chars):\n",
        "\tin_text = seed_text\n",
        "\t# generate a fixed number of characters\n",
        "\tfor _ in range(n_chars):\n",
        "\t\t# encode the characters as integers\n",
        "\t\tencoded = [mapping[char] for char in in_text]\n",
        "\t\t# truncate sequences to a fixed length\n",
        "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "\t\t# one hot encode\n",
        "\t\tencoded = to_categorical(encoded, num_classes=len(mapping))\n",
        "\t\t# predict character\n",
        "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
        "\t\t# reverse map integer to character\n",
        "\t\tout_char = ''\n",
        "\t\tfor char, index in mapping.items():\n",
        "\t\t\tif index == yhat:\n",
        "\t\t\t\tout_char = char\n",
        "\t\t\t\tbreak\n",
        "\t\t# append to input\n",
        "\t\tin_text += char\n",
        "\treturn in_text  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHH_I5QiUxnY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "outputId": "86c2e09e-eb7f-4d65-a0ec-06cc1a3344a0"
      },
      "source": [
        "# pick a random seed\n",
        "start_index = numpy.random.randint(0, 50000)\n",
        "pattern = dataX[start_index]\n",
        "input_str = ''.join([int_to_char[value] for value in pattern])\n",
        "\n",
        "print(\"Given Sequence: \\n\", input_str)\n",
        "answer_char = generate_seq_char_based(char_model1,mapping,50,input_str,10)  \n",
        "print(\"Predict Sequence: \\n\",answer_char)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Given Sequence: \n",
            " said the mouse, getting up and walking away.\n",
            "UNKyou \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-120-49a17c2ad50b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Given Sequence: \\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0manswer_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_seq_char_based\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar_model1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmapping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_str\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Predict Sequence: \\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0manswer_char\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-116-10c357a318ec>\u001b[0m in \u001b[0;36mgenerate_seq_char_based\u001b[0;34m(model, mapping, seq_length, seed_text, n_chars)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_chars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0;31m# encode the characters as integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                 \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0min_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m                 \u001b[0;31m# truncate sequences to a fixed length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncating\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pre'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-116-10c357a318ec>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_chars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0;31m# encode the characters as integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                 \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0min_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m                 \u001b[0;31m# truncate sequences to a fixed length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncating\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pre'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'U'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iutpuJAgrgU8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd03fba7-e8ef-4085-d978-442e6a1a27d8"
      },
      "source": [
        "input_str = \"The boy laughed at the fright he had caused. This time, the villagers left angrily. The third day, as the boy went up\\\n",
        " the small hill, he suddenly saw a wolf attacking his sheep. He cried as hard as he could, “Wolf! Wolf! Wolf!”, but not \\\n",
        " a single villager came to help him. The villagers thought that he was trying to fool them again and did not come to rescue \\\n",
        " him or his sheep.\"\n",
        "\n",
        " # Use first 100 characeters from given input_str as input and generate next 200 characters.\n",
        "\n",
        " \n",
        "str2 = predict_next_100_chars(input_str,200)\n",
        "print(\"Predict Sequence: \\n\",str2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Given Sequence: \n",
            " The boy laughed at the fright he had caused. This time, the villagers left angrily. The third day, as the boy went up the small hill, he suddenly saw a wolf attacking his sheep. He cried as hard as he could, “Wolf! Wolf! Wolf!”, but not  a single villager came to help him. The villagers thought that he was trying to fool them again and did not come to rescue  him or his sheep.\n",
            "Predict Sequence: \n",
            " The boy laughed at the fright he had caused. This time, the villagers left angrily. The third day, as the boy went up the small hill, he suddenly saw a wolf attacking his sheep. He cried as hard as he could, “Wolf! Wolf! Wolf!”, but not a single villager came to help him. The villagers thought that he was trying to fool them again and did not come to rescue him or his sheep.u c h e s s ,   “ a n d   t h e   m o r a l   o f   t h a t   i s — ‘ t h e   m o r e   t h a n   t h e   r e a s o n   i s   t h e   m o u s e   d i d   y o u   t o   d o   t o   d o   t h a t   i  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8shbcukr0tJ"
      },
      "source": [
        "## Character based LSTM Model 2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWBPCrTdr46U"
      },
      "source": [
        "model1 = Sequential()\n",
        "model1.add(Embedding(n_vocab, embedding_dim, input_length=max_length))\n",
        "model1.add(LSTM(256, input_shape=(X.shape[1], embedding_dim),return_sequences=True))\n",
        "model1.add(Dropout(0.2))\n",
        "model1.add(LSTM(256))\n",
        "model1.add(Dropout(0.2))\n",
        "model1.add(Dense(y.shape[1], activation='softmax'))\n",
        "model1.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZxrtjFIr63L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba14e9c2-2906-4bc7-b985-0df93bc1a324"
      },
      "source": [
        "model1.fit(X, y, epochs=20, batch_size=64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "2225/2225 [==============================] - 50s 22ms/step - loss: 2.1236\n",
            "Epoch 2/20\n",
            "2225/2225 [==============================] - 49s 22ms/step - loss: 1.5672\n",
            "Epoch 3/20\n",
            "2225/2225 [==============================] - 50s 22ms/step - loss: 1.3843\n",
            "Epoch 4/20\n",
            "2225/2225 [==============================] - 50s 23ms/step - loss: 1.2818\n",
            "Epoch 5/20\n",
            "2225/2225 [==============================] - 50s 22ms/step - loss: 1.2139\n",
            "Epoch 6/20\n",
            "2225/2225 [==============================] - 50s 22ms/step - loss: 1.1612\n",
            "Epoch 7/20\n",
            "2225/2225 [==============================] - 50s 22ms/step - loss: 1.1142\n",
            "Epoch 8/20\n",
            "2225/2225 [==============================] - 49s 22ms/step - loss: 1.0775\n",
            "Epoch 9/20\n",
            "2225/2225 [==============================] - 49s 22ms/step - loss: 1.0430\n",
            "Epoch 10/20\n",
            "2225/2225 [==============================] - 50s 22ms/step - loss: 1.0152\n",
            "Epoch 11/20\n",
            "2225/2225 [==============================] - 49s 22ms/step - loss: 0.9869\n",
            "Epoch 12/20\n",
            "2225/2225 [==============================] - 49s 22ms/step - loss: 0.9666\n",
            "Epoch 13/20\n",
            "2225/2225 [==============================] - 49s 22ms/step - loss: 0.9459\n",
            "Epoch 14/20\n",
            "2225/2225 [==============================] - 49s 22ms/step - loss: 0.9281\n",
            "Epoch 15/20\n",
            "2225/2225 [==============================] - 49s 22ms/step - loss: 0.9130\n",
            "Epoch 16/20\n",
            "2225/2225 [==============================] - 49s 22ms/step - loss: 0.9008\n",
            "Epoch 17/20\n",
            "2225/2225 [==============================] - 49s 22ms/step - loss: 0.8858\n",
            "Epoch 18/20\n",
            "2225/2225 [==============================] - 49s 22ms/step - loss: 0.8751\n",
            "Epoch 19/20\n",
            "2225/2225 [==============================] - 49s 22ms/step - loss: 0.8634\n",
            "Epoch 20/20\n",
            "2225/2225 [==============================] - 49s 22ms/step - loss: 0.8577\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff745834828>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6RTfHN6r8wh"
      },
      "source": [
        "# Generate the sequence similar to above methods\n",
        "\n",
        "'''\n",
        "Complete code below to get the generated string using the model.\n",
        "'''\n",
        "def predict_next_100_chars(pattern,x):\n",
        "\tprint(\"Given Sequence: \\n\", pattern)\n",
        "\tsequence = np.array(char_tokenizer.texts_to_sequences(pattern)[:100]).reshape(1,100)\n",
        "\tfor i in range(x):\n",
        "\t\tprediction = model1.predict(sequence, verbose=0)\n",
        "\t\tsequence = np.append(sequence,prediction.argmax())\n",
        "\t\tsequence = sequence[1:]\n",
        "\t\tsequence.resize(1,100)\n",
        "\t\tfinal_string = ' '.join(pattern.split()[:100]) +  ''.join(char_tokenizer.sequences_to_texts(sequence))\n",
        "\treturn final_string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CKJXZ4tYmL0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d9f3098-554a-4988-8486-fa1a032b009e"
      },
      "source": [
        "# pick a random seed\n",
        "start_index = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start_index]\n",
        "input_str = ''.join([int_to_char[value] for value in pattern])\n",
        "\n",
        "answer2 = predict_next_100_chars(input_str,200)\n",
        "print(\"Predict Sequence: \\n\",answer2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Given Sequence: \n",
            " rply; “i advise you to leave off this minute.” she generally gave herself very good advice, (though \n",
            "Predict Sequence: \n",
            " rply; “i advise you to leave off this minute.” she generally gave herself very good advice, (thoughg   t o   b e   s u r p r i s e d   t o   s e e   t h a t   s h e   w a s   n o w   a b o u t   i t ,   a n d   t h e   w h o l e   p a r t y   w a s   t h e   w h i t e   r a b b i t   w a s   s i t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyRCfLe5YmL7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ffa6155-c907-42e8-c0a6-516e9c74d95b"
      },
      "source": [
        "input_str = \"The boy laughed at the fright he had caused. This time, the villagers left angrily. The third day, as the boy went up\\\n",
        " the small hill, he suddenly saw a wolf attacking his sheep. He cried as hard as he could, “Wolf! Wolf! Wolf!”, but not \\\n",
        " a single villager came to help him. The villagers thought that he was trying to fool them again and did not come to rescue \\\n",
        " him or his sheep.\"\n",
        "\n",
        " # Use first 100 characeters from given input_str as input and generate next 200 characters.\n",
        "\n",
        "str1 = predict_next_100_chars(input_str,200)\n",
        "print(\"Predict Sequence: \\n\",str1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Given Sequence: \n",
            " The boy laughed at the fright he had caused. This time, the villagers left angrily. The third day, as the boy went up the small hill, he suddenly saw a wolf attacking his sheep. He cried as hard as he could, “Wolf! Wolf! Wolf!”, but not  a single villager came to help him. The villagers thought that he was trying to fool them again and did not come to rescue  him or his sheep.\n",
            "Predict Sequence: \n",
            " The boy laughed at the fright he had caused. This time, the villagers left angrily. The third day, as the boy went up the small hill, he suddenly saw a wolf attacking his sheep. He cried as hard as he could, “Wolf! Wolf! Wolf!”, but not a single villager came to help him. The villagers thought that he was trying to fool them again and did not come to rescue him or his sheep.e   t h a t   s h e   w a s   n o w   a b o u t   i t ,   a n d   t h e   w h o l e   p a r t y   w a s   t h e   w h i t e   r a b b i t   w a s   s i t t i n g   o n   t h e   d o o r ,   s h e   w\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPrjxjoNsaQC"
      },
      "source": [
        "# Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kW5_IlBeZAPq"
      },
      "source": [
        "**Question:** What are your observations based on the model(all) outputs on train data(in domain) vs unseen data(out of domain) ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YryzP1upZv5h"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "**Answer:** Word based LSTM model performs better on seen data as we can see in epoch 20 accuracy score is better then val_accuracy but also it is performing well in unseen data as well. And also models can approximate to the past data pretty well. But for the new data it is not perfect as on train data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBzCD0I0Z3uP"
      },
      "source": [
        "**Question:** What did you observe in the outputs of char LSTM model1 vs char LSTM model2 ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUHxHmXZaNdn"
      },
      "source": [
        "**Answer:**  The Observation of output:\n",
        "LSTM model2 is performing way better than model1. As in the model 1 same words are repeating and model2 sentence makes more sense as model2 is more complex and adds one extra unit than model1"
      ]
    }
  ]
}